# -*- coding: utf-8 -*-
"""22_8_Final Stroke Prediction_Valentine_Submission.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qZHDAf_SX-iBy2VTyiDlst9OEiL-8csi
"""

# Commented out IPython magic to ensure Python compatibility.
#import the required package
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split, cross_val_predict
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix, make_scorer

#from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn import metrics


# Libraries to Hyperparameter Tuning
#from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, loguniform, uniform


# To filter Warnings
import warnings

# %matplotlib inline
sns.set()

dataframe = pd.read_csv("Transformed_Stroke_Dataset_Updated_v2.csv")

#checking the data head
dataframe.head()

dataframe = pd.read_csv("Transformed_Stroke_Dataset_Updated_2.csv")

#checking the data head
df = dataframe.head()

# checking the dimension
dataframe.shape

def detect_noise_zscore(dataframe, threshold=3):
    z_scores = np.abs(stats.zscore(dataframe.select_dtypes(include=[np.number])))
    noise = (z_scores > threshold).sum(axis=1)
    noise_data = dataframe[noise > 0]
    return noise_data

def detect_noise_iqr(dataframe):
    Q1 = dataframe.quantile(0.25)
    Q3 = dataframe.quantile(0.75)
    IQR = Q3 - Q1
    noise = ((dataframe < (Q1 - 1.5 * IQR)) | (dataframe > (Q3 + 1.5 * IQR))).sum(axis=1)
    noise_data = dataframe[noise > 0]
    return noise_data

noise_iqr = detect_noise_iqr(df)
print("Noise detected using IQR:")
print(noise_iqr)

def cap_outliers(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

return series.apply(lambda x: lower_bound if x < lower_bound else upper_bound if x > upper_bound else x)

for col in df.select_dtypes(include=['float64', 'int64']).columns:
    df[col] = cap_outliers(df[col])

# Check the results
print(df.describe())

dataframe.describe().round(2)

dataframe.info()

# checking the number of duplicated variable
dataframe.duplicated().sum()

# checking the number of null value in each features/column
dataframe.isnull().sum()

#checking the unique variable in each features
dataframe.nunique()

def plot_columns(column_names, data=dataframe):
    """
    Function to plot the distribution of categorical variables in the DataFrame.

    Parameters:
    data : DataFrame to be analyzed.
    column_names (list): Names of the columns to plot.

    Returns: output/shows a sns count plot for each passed column/variable
    """
    for col in column_names:
        sns.countplot(x=data[col])
        plt.title('Distribution of ' + col)
        plt.show()

# calling the count plot function with required argument(s) i.e. variables
plot_columns(['gender', 'stroke', 'Residence_type', 'work_type', 'hypertension', 'heart_disease', 'smoking_status'])

#Bi-variate EDA with target variable and gender
sns.countplot(x="gender", hue="stroke", data=dataframe)

def check_target_distribution_by_gender(data, target, gender_col='gender'):
    """
    Function to check the distribution of a binary target variable in a dataset by gender.

    Parameters:
    - data (pandas.DataFrame): Dataset to be analyzed.
    - target (str): Name of the binary target variable.
    - gender_col (str): Name of the column that contains gender information.

    Returns:
    - % of the gender in the traget variable
    - count of the responses in the target varaible
    - % of each response of the target varaible
    - A statement to confirm if the data is balance or not
    """

    # Calculate the percentage of samples with the target variable for each gender
    female_with_target = data[data[gender_col] == 'Female'][target].sum() / data[data[gender_col] == 'Female'][target].count() * 100
    male_with_target = data[data[gender_col] == 'Male'][target].sum() / data[data[gender_col] == 'Male'][target].count() * 100
    other_gender_with_target = data[data[gender_col] == 'Other'][target].sum() / data[data[gender_col] == 'Other'][target].count() * 100

    print("% of Female with {}: {:2.2f}%".format(target, female_with_target))
    print("% of Male with {}: {:2.2f}%".format(target, male_with_target))
    print("% of Other gender with {}: {:2.2f}%".format(target, other_gender_with_target))
    print('*' * 30)

    # Check the distribution of the target variable for the whole dataset
    target_counts = data[target].value_counts()
    print(target_counts)
    print('*' * 30)

    # Calculate the percentage of samples in each class
    percentage_target = target_counts[1] / len(data) * 100
    percentage_no_target = target_counts[0] / len(data) * 100
    print('Percentage of {} cases: {:.2f}%'.format(target, percentage_target))
    print('Percentage of non-{} cases: {:.2f}%'.format(target, percentage_no_target))
    print('*' * 30)

    if percentage_target == percentage_no_target:
        print('The dataset is balanced. Oversampling or undersampling may not be required.')
    else:
        print('The dataset is imbalanced. Oversampling or undersampling may be required.')




# calling the function with required argument(s)
check_target_distribution_by_gender(dataframe, 'stroke')

#function to plot numeric variables

def plot_numeric_data(df, x_vars, figsize=(15, 13)):
    """
    Function plot box plot and histogram for the numeric variables.

    Parameters:
    - data (pandas.DataFrame): Dataset to be analyzed.

    - x_vars (list): list of numeric varibales.

    Returns:
    - A box plot and histogram for each variables passed

    """
    fig, ax = plt.subplots(len(x_vars), 2, figsize=figsize)
    for i, var in enumerate(x_vars):
        sns.boxplot(x=df[var], ax=ax[i, 0])
        sns.histplot(df[var], kde=True, ax=ax[i, 1])
    plt.tight_layout()


# calling the function with required argument(s)
plot_numeric_data(dataframe, ['age', 'avg_glucose_level', 'bmi'])

#function to plot pairplot numeric variables
def create_pairplot(data):
    """
    Function to plot pairplot for all variables

    Parameters:
    - data (pandas.DataFrame): Dataset to be analyzed.

    Returns:
    - A pairplot that shows pairwise relationships between variables in the dataset

    """
    sns.set(rc={'figure.figsize':(11.7,8.27)})
    df_attr = data.iloc[:, 1:-1] # Select all columns except the id and target (the first and last)variable
    sns.pairplot(df_attr, diag_kind='kde')


# calling the function with required argument(s)
create_pairplot(dataframe)

# heatmap function
def create_heatmap(data):
    """
    Function to heatmap between the variables in the dataset

    Parameters:
    - data (pandas.DataFrame): Dataset to be analyzed.

    Returns:
    - Plot a Heatmap of relationships between variables in the dataset

    """
    plt.figure(figsize=(10,6))
    sns.heatmap(data.iloc[:, 1:].corr(), cmap=plt.cm.Reds, annot=True)
    plt.title('Heatmap displaying the relationship between the features of the data', fontsize=13)
    plt.show()


# calling the function with required argument(s)
# the heatmap with show result for numerical variables as the the cat. variables are yet to be converted
create_heatmap(dataframe)

#creating a copy of the dataframe
df_stroke = dataframe.copy()

#removing the null value from the bmi column
df_stroke = df_stroke.dropna(subset=['bmi'])

# Remove column name 'id'
df_stroke = df_stroke.drop(['id'], axis=1)

# checking the number of null value in each features/column
df_stroke.isnull().sum()

#encoding the categorical variabel into integer
varList = df_stroke.select_dtypes(include = "object").columns

le = LabelEncoder()

for i in varList:
    df_stroke[i] = le.fit_transform(df_stroke[i].astype(str))

df_stroke.info()

df_stroke.head()

# calling the heatmap function with required argument(s) to display corrolation for all variables
create_heatmap(df_stroke)

# calling the pairplot function with required argument(s)
create_pairplot(df_stroke)

# Split the data into input features and target variable
X = df_stroke.drop(['stroke'], axis=1)
y = df_stroke['stroke']

# Split the data into training and testing sets 80:20
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Scale the predictor using StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Use SMOTE for oversampling
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

# Print class distribution of original and resampled data
print('Class distribution before resampling:', y_train.value_counts())
print("*" * 60)
print('Class distribution after resampling:', y_train_resampled.value_counts())

def model_evaluator(model, X_train, y_train, X_test, y_test):
    """
    Train a model, test it on test data, and print evaluation metrics.
    Also plot a confusion matrix.

    Parameters:
    - model : a scikit-learn model object
    - X_train : training feature data
    - y_train : training target data
    - X_test : test feature data
    - y_test : test target data

    Returns:
    - None
    """
    # Train the model on the training data
    model.fit(X_train, y_train)

    # Test the model on the test data
    y_pred = model.predict(X_test)

    # Compute evaluation metrics accuracy, sensitivity and specificity
    cm = confusion_matrix(y_test, y_pred)
    TN, FP, FN, TP = cm.ravel()
    model_accuracy = (TP+TN)/(TP+TN+FP+FN)
    model_recall = TP/(TP+FN)
    model_specificity = TN/(TN+FP)

    df = pd.DataFrame({'Accuracy': [model_accuracy], 'Sensitivity/Recall': [model_recall], 'Specificity': [model_specificity]})
    display(df.style.hide(axis='index'))


	# Plot confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, cmap='Blues_r', fmt='g')
    plt.title('Confusion matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

# Create a model object for Random Forest Classifier
rfc_model = RandomForestClassifier(random_state=42)

print("Random Forest Base Model Metrics and Confusion Matrix")
# Train and test the model
model_evaluator(rfc_model, X_train_resampled, y_train_resampled, X_test_scaled, y_test)

# Create a model object for Logistic regression
lr_model = LogisticRegression()

print("Logistic Regression Base Model Metrics and Confusion Matrix")
# Train and test the model
model_evaluator(lr_model, X_train_resampled, y_train_resampled, X_test_scaled, y_test)

# Create a model object for Support Vector Classifier
svc_model = SVC(random_state=42)


print("Support Vector Classifier Base Model Metrics and Confusion Matrix")
# Train and test the model
model_evaluator(svc_model, X_train_resampled, y_train_resampled, X_test_scaled, y_test)

# Define the hyperparameter grid to be use in tuning RFC
param_dist = {'bootstrap': [True, False],
              'n_estimators': randint(10, 1000),
              'max_depth': randint(1, 100),
              'min_samples_split': randint(2, 50),
              'min_samples_leaf': randint(1, 50),
              'max_features': ['auto', 'sqrt', 'log2', None]}


# Define the RFC Random Search model with 5-fold cross-validation
random_search = RandomizedSearchCV(RandomForestClassifier(),
                                   param_distributions=param_dist,
                                   n_iter=100,
                                   cv=5,
                                   n_jobs=-1,
                                   verbose=3,
                                   random_state=42)

# Fit the random search object to the data
random_search.fit(X_train_resampled, y_train_resampled)

#Print the best RFC parameters
random_search.best_params_

#passing the best parameter values to the RFC model
rfc = RandomForestClassifier(bootstrap=False,
                             n_estimators=313,
                             min_samples_leaf=2,
                             min_samples_split=2,
                             max_features='auto',
                             max_depth=60)

print("Random Forest Tunned Model Metrics and Confusion Matrix")

#calling the model_evaluater with the tunned RFC
model_evaluator(rfc, X_train_resampled, y_train_resampled, X_test_scaled, y_test)

# Define the hyperparameter grid to be used in tuning LR
grid = dict()
grid['penalty'] = ['l1', 'l2', 'elasticnet', 'none']
grid['C'] = loguniform(1e-5, 100)
grid['solver'] = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']
grid['max_iter'] = [100, 1000, 10000]

# Define the LR Random Search model with 5-fold cross-validation
lr_search = RandomizedSearchCV(LogisticRegression(),
                            grid,
                            n_iter=100,
                            error_score=0,
                            verbose=3,
                            n_jobs=-1,
                            cv=5,
                            random_state=42)

# Fit the random search object to the data
lr_search.fit(X_train_resampled, y_train_resampled)

#getting the best parameter for LR
lr_search.best_params_

#setting up an instance of LR witht the best parameter
lr = LogisticRegression(penalty='l1',
                        solver='saga',
                        C=0.0015866892068163172,
                        max_iter=10000,
                        random_state=42)

print("Logistic Regression Tunned Model Metrics and Confusion Matrix")

#calling the model_evaluater with the tunned RFC
model_evaluator(lr, X_train_resampled, y_train_resampled, X_test_scaled, y_test)

# Define the parameter grid for Random Search on SVC
param_distributions = {'C': uniform(loc=0, scale=10),
                       'gamma': ['scale', 'auto'],
                       'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
                       'degree': [2, 3, 4],
                       'coef0': uniform(loc=0, scale=5)}

# Define the SVC Random Search model with 5-fold cross-validation
random_search_svc = RandomizedSearchCV(estimator=SVC(),
                                       param_distributions=param_distributions,
                                       n_iter=100,
                                       cv=5,
                                       n_jobs=-1,
                                       verbose=3,
                                       random_state=42)


# Fit the random search object to the data
random_search_svc.fit(X_train_resampled, y_train_resampled)

#Getting the best SVC parameters
random_search_svc.best_params_

#define the best random search parameter to SVC model
svc = SVC(C=6.350936508676438,
    	coef0=0.2265200488602226,
          degree=4,
          gamma='auto',
          kernel='poly')




print("Support Vector Machine Tunned Model Metrics and Confusion Matrix")
#calling the model_evaluater with the tunned RFC
model_evaluator(svc, X_train_resampled, y_train_resampled, X_test_scaled, y_test)

The results of the current investigation demonstrate the effectiveness of the two models—Random Forest and Logistic Regression—in predicting the prevalence of stroke based on variables linked to demographics and health.
Fortunately the Logistic Regression model seems to be a better choice in terms of sensitivity and specificity, since it achieves a respectably high score across the selected metrics.
Scores of 96%, 97%, and 95% were obtained by the LR base model in terms of accuracy, sensitivity, and specificity.
Because Logistic Regression has such high accuracy, sensitivity, and specificity, the study suggests it is the best model for predicting stroke in high-risk patients in Abuja, Nigeria.